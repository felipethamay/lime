{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "closing-symphony",
   "metadata": {},
   "source": [
    "## Pré-processamento de texto e divisão de dados de treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-whale",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "macro-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-master",
   "metadata": {},
   "source": [
    "#### Imports pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "waiting-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-atmosphere",
   "metadata": {},
   "source": [
    "### Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "helpful-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/imdb-reviews-pt-br.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-serum",
   "metadata": {},
   "source": [
    "### Manipulação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unauthorized-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['id']\n",
    "del df['text_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fuzzy-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df.copy()\n",
    "df_transformed.sentiment = df_transformed['sentiment'].map({'pos': 1, 'neg': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dense-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrigir_nomes(nome):\n",
    "    nome = nome.replace(r'[,;&!?/.:]', ' ').replace('(\\\\d|\\\\W)+|\\w*\\d\\w*', ' ').replace(';', ' ').replace(':', ' ').replace(',', ' ').replace('\"', ' ').replace('ç', 'c').replace('é', 'e').replace('ç', 'c').replace('ã', 'a').replace('ê', 'e').replace('í', 'i').replace('á', 'a').replace('ó', 'o').replace('ú', 'u').replace('â', 'a').replace('ô', 'o').replace(' o ', ' ').replace(' a ', ' ').replace(' os ', ' ').replace(' as ', ' ').replace(' um ', ' ').replace(' uma ', ' ').replace(' uns ', ' ').replace(' umas ', ' ').replace(' ao ', ' ').replace(' aos ', ' ').replace(' à ', ' ').replace(' às ', ' ').replace(' da ', ' ').replace(' das ', ' ').replace(' do ', ' ').replace(' dos ', ' ').replace(' na ', ' ').replace(' nas ', ' ').replace(' no ', ' ').replace(' num ', ' ').replace(' numas ', ' ')\n",
    "    return nome\n",
    "\n",
    "df_transformed['text_pt'] = df_transformed['text_pt'].apply(corrigir_nomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "italian-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['text_pt'] = df_transformed['text_pt'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accepting-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['text_pt'] = df_transformed['text_pt'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interim-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_words = df_transformed['text_pt']\n",
    "sbs = SnowballStemmer(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demanding-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in e_words:\n",
    "    rootWord = sbs.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exempt-screw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_pt_without_stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mais vez  sr. costner arrumou filme por muito ...</td>\n",
       "      <td>0</td>\n",
       "      <td>mais vez sr. costner arrumou por muito mais te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>este e exemplo motivo pelo qual maioria filmes...</td>\n",
       "      <td>0</td>\n",
       "      <td>este exemplo motivo pelo qual maioria mesmos. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>primeiro de tudo eu odeio esses raps imbecis  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>primeiro tudo eu odeio raps imbecis poderiam a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nem mesmo beatles puderam escrever musicas que...</td>\n",
       "      <td>0</td>\n",
       "      <td>nem beatles puderam escrever musicas todos gos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>filmes de fotos de latao nao e palavra apropri...</td>\n",
       "      <td>0</td>\n",
       "      <td>fotos latao palavra apropriada verdade tanto o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_pt  sentiment  \\\n",
       "0  mais vez  sr. costner arrumou filme por muito ...          0   \n",
       "1  este e exemplo motivo pelo qual maioria filmes...          0   \n",
       "2  primeiro de tudo eu odeio esses raps imbecis  ...          0   \n",
       "3  nem mesmo beatles puderam escrever musicas que...          0   \n",
       "4  filmes de fotos de latao nao e palavra apropri...          0   \n",
       "\n",
       "                            text_pt_without_stemming  \n",
       "0  mais vez sr. costner arrumou por muito mais te...  \n",
       "1  este exemplo motivo pelo qual maioria mesmos. ...  \n",
       "2  primeiro tudo eu odeio raps imbecis poderiam a...  \n",
       "3  nem beatles puderam escrever musicas todos gos...  \n",
       "4  fotos latao palavra apropriada verdade tanto o...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed['text_pt_without_stemming'] = df_transformed['text_pt'].apply(lambda x: ' '.join([word for word in x.split() if word not in (rootWord)]))\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "white-novelty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"portuguese\"))\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incoming-exposure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\felipe\\anaconda3\\envs\\checklist3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_pt_without_stemming</th>\n",
       "      <th>text_pt_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mais vez  sr. costner arrumou filme por muito ...</td>\n",
       "      <td>0</td>\n",
       "      <td>mais vez sr. costner arrumou por muito mais te...</td>\n",
       "      <td>vez sr. costner arrumou filme tempo necessario...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>este e exemplo motivo pelo qual maioria filmes...</td>\n",
       "      <td>0</td>\n",
       "      <td>este exemplo motivo pelo qual maioria mesmos. ...</td>\n",
       "      <td>exemplo motivo maioria filmes acao sao mesmos....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>primeiro de tudo eu odeio esses raps imbecis  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>primeiro tudo eu odeio raps imbecis poderiam a...</td>\n",
       "      <td>primeiro tudo odeio raps imbecis nao poderiam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nem mesmo beatles puderam escrever musicas que...</td>\n",
       "      <td>0</td>\n",
       "      <td>nem beatles puderam escrever musicas todos gos...</td>\n",
       "      <td>beatles puderam escrever musicas todos gostass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>filmes de fotos de latao nao e palavra apropri...</td>\n",
       "      <td>0</td>\n",
       "      <td>fotos latao palavra apropriada verdade tanto o...</td>\n",
       "      <td>filmes fotos latao nao palavra apropriada verd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_pt  sentiment  \\\n",
       "0  mais vez  sr. costner arrumou filme por muito ...          0   \n",
       "1  este e exemplo motivo pelo qual maioria filmes...          0   \n",
       "2  primeiro de tudo eu odeio esses raps imbecis  ...          0   \n",
       "3  nem mesmo beatles puderam escrever musicas que...          0   \n",
       "4  filmes de fotos de latao nao e palavra apropri...          0   \n",
       "\n",
       "                            text_pt_without_stemming  \\\n",
       "0  mais vez sr. costner arrumou por muito mais te...   \n",
       "1  este exemplo motivo pelo qual maioria mesmos. ...   \n",
       "2  primeiro tudo eu odeio raps imbecis poderiam a...   \n",
       "3  nem beatles puderam escrever musicas todos gos...   \n",
       "4  fotos latao palavra apropriada verdade tanto o...   \n",
       "\n",
       "                           text_pt_without_stopwords  \n",
       "0  vez sr. costner arrumou filme tempo necessario...  \n",
       "1  exemplo motivo maioria filmes acao sao mesmos....  \n",
       "2  primeiro tudo odeio raps imbecis nao poderiam ...  \n",
       "3  beatles puderam escrever musicas todos gostass...  \n",
       "4  filmes fotos latao nao palavra apropriada verd...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.colums = ['text_pt']\n",
    "\n",
    "df_transformed['text_pt_without_stopwords'] = df_transformed['text_pt'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-dakota",
   "metadata": {},
   "source": [
    "### Divisão treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acoustic-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df_transformed,\n",
    "    test_size = 0.3, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-theta",
   "metadata": {},
   "source": [
    "#### Salvando dados de test e train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "increasing-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('./data_train_test/df_train_imbd.csv')\n",
    "df_test.to_csv('./data_train_test/df_test_imbd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-bolivia",
   "metadata": {},
   "source": [
    "#### Lendo dados de test e train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "affected-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data_train_test/df_train_imbd.csv')\n",
    "df_test = pd.read_csv('./data_train_test/df_test_imbd.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
